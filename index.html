<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Performance study of HEP data analysis tools</title>
  <link rel="stylesheet" href="poster.css">
  <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1">
  <!-- Based on a poster template from https://github.com/cpitclaudel/academic-poster-template -->

  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link
    href="https://fonts.googleapis.com/css2?family=Fira+Sans+Condensed:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap"
    rel="stylesheet">

  <style type="text/css">
    html {
      font-size: 1.15rem
    }
  </style>
</head>

<body vocab="http://schema.org/" typeof="ScholarlyArticle">
  <header role="banner">
    <aside>
      <!-- <a href="https://github.com/cpitclaudel/academic-poster-template"><img src="logo.svg" alt="Tutorial logo"></a> -->
    </aside>
    <div>
      <h1 property="headline">A performance study of HEP data analysis tools</h1>
      <!-- <h2 property="alternativeHeadline">
        Project carried out at INFN-Sezione di Bari (June - August 2022)
        as part of the DOE-INFN Summer Exchange Program 2022
      </h2> -->
      <!-- <h2 property="alternativeHeadline">
        Parallel ROOT I/O with Uproot and RDataFrame
      </h2> -->
      <address>
        <a property="author">Dung Hoang<sup>a</sup></a><a property="author">, Adriano Di Florio<sup>b, d</sup></a></a><a
          property="author">, Alexis Pompili<sup>b, c</sup></a></a><a property="author">, Umit Sozbilir<sup>b,
            c</sup></a></a><a property="author">, Vincenzo Mastrapasqua<sup>b, c</sup></a>
        <br /> <sup>a</sup><a property="sourceOrganization">Rhodes College,&nbsp</a><sup>b</sup><a
          property="sourceOrganization">INFN-Bari,&nbsp</a><sup>c</sup><a property="sourceOrganization">Università degli
          studi di Bari Aldo Moro,&nbsp</a><sup>d</sup><a property="sourceOrganization">Politecnico di Bari</a>
      </address>
      <span class="publication-info">
        <span property="publisher">Unpublished</span>,
        <time pubdate property="datePublished" datetime="2020-09-08">September 8, 2020</time>
      </span>
    </div>
    <aside>
      <!-- <a href="https://csail.mit.edu"><img style="background: white" src="csail.png" alt="MIT CSAIL Logo"></a> -->
    </aside>
  </header>

  <main property="articleBody">
    <article property="abstract">
      <header>
        <h3>Introduction</h3>
      </header>

      <p>
        The Python programming language is currently characterized by an enormous and rapidly growing “ecosystem” of
        software tools that support the analysis and visualization of “Big Data,” datasets that are too large or
        complex to be dealt with by traditional data-processing methods. Examples include <a
          href="https://numpy.org/">Numpy</a> (numerical
        computing), <a href="https://pandas.pydata.org/">pandas</a> (data manipulation), <a
          href="https://matplotlib.org/">matplotlib</a> (visualization), and <a
          href="https://www.tensorflow.org/">tensorflow</a> (machine learning). With the huge and increasingly complex
        volume of data that large-scale High Energy Physics (HEP) experiments
        produce, a complete HEP
        data analysis workflow can provide great benefits.
      </p>
      <p>
        In this project, we explored different Python-based
        approaches to deal with the
        standard ROOT format common to all HEP data. With reference to a representative HEP workflow taken as use case,
        we measured the performance of <strong><a href="https://uproot.readthedocs.io/en/latest/">Uproot</a></strong>, a
        library for reading and writing ROOT
        files in pure Python and NumPy, and of <strong><a
            href="https://root.cern/doc/master/classROOT_1_1RDataFrame.html">RDataFrame</a></strong>, the modern ROOT's
        high-level interface for efficient data
        analysis. To match the performance of a traditional C++ ROOT workflow, we also adopted parallel processing in
        our study.
      </p>
    </article>

    <article property="abstract">
      <header>
        <h3>Data and computing resources</h3>
      </header>
      <p>
        The ROOT files used in our study, which have a total size of about 128GB, were taken from CMS Open Data.
      </p>
      <p>
        Measurements were run on three different machines on the ReCaS-Bari computing center:
      </p>

      <ol>
        <li>
          <strong>wn-gpu-8-3-22</strong> (256 CPUs, ~2000 GB RAM)
        </li>
        <li>
          <strong>wn-1-8-9</strong> (64 CPUs, ~250GB RAM)
        </li>
        <li>
          <strong>tesla04</strong> (32 CPUs, ~250 GB RAM)
        </li>
      </ol>

      <p>
        For the first two machines, data could only be stored remotely on the computing cluster, while on
        <strong>tesla04</strong>, it
        was possible to access the data on the local disk.
      </p>
      <p>
        <strong>tesla04</strong> was also the only computer on which we could install the latest version of ROOT (6.26).
        To make fair comparison between different computers, therefore, all measurements were run with ROOT 6.24 unless
        otherwise specified.
      </p>

    </article>

    <article property="abstract">
      <header>
        <h3>Analysis task</h3>
      </header>

      <p>
        Specifically we measure the total runtime of the sequence of the following operations:
      <ol>
        <li> accessing the TTree inside the input ROOT files; </li>
        <li> applying specific filters (selection criteria) on the variables stored in the TTree, in order to extract a
          known physical signal associated to the decay chain of a beauty meson (B0s → J/psi phi, J/psi→mumu, phi→KK);
        </li>
        <li> converting the TBranch containing the selected invariant mass to a NumPy array for further analysis task
          (signal fitting, etc ...). </li>
      </ol>
      </p>

      <figure>
        <img src="plots/dist1.png" style="width:90%">
      </figure>
      <figure>
        <img src="plots/dist2.png" style="width:90%">
      </figure>

    </article>

    <article property="abstract">
      <header>
        <h3>Parallel processing</h3>
      </header>

      <p>
        Instead of letting one CPU handle the whole dataset, we used multiple CPUs running in parallel to process pieces
        of data concurrently.
      </p>
      <p>
        Without parallelism, the time it takes to complete the analysis task would be:
      </p>

      \[T = t_u\cdot S\]

      <p>
        Where T is the total run time, t<sub>u</sub> is the time to process one unit of data, and S is the size of data.
      </p>

      <p>
        By implementing parallel processing, this equation becomes:
      </p>

      \[ T = C(S, P) + t_u\cdot \lceil{S \over P}\rceil \]

      <p>
        Where C(S, P) is the time to access/split/merge the data and P is the number of concurrent processes. The
        fraction
        S/P is rounded up to make sure that no data is lost.
      </p>

    </article>

    <article property="abstract">
      <header>
        <h3>Parallel processing with Uproot</h3>
      </header>

      <p>
        Uproot does not provide a built-in option for implicit parallel processing. To enable parallelism, therefore,
        we manually split the data into smaller chunks and then created subprocesses with Python’s multiprocessing
        module to handle them. Since Uproot allows users to specify the number of events to be processed in each TTree,
        the smallest unit of data assigned to each subprocess is one event. As a result, data can be distributed
        very evenly among subprocesses, and the runtime function for Uproot would be:
      </p>

      \[ T = C(S, P) + t_e\cdot \lceil{S_e \over P}\rceil \]

      <p>
        Where t<sub>e</sub> is the time to process one <strong>event</strong>, and S<sub>e</sub> is number of
        <strong>events</strong> in the dataset.
      </p>

    </article>

    <article property="abstract">
      <header>
        <h3>Parallel processing with RDataFrame</h3>
      </header>

      <p>
        RDataFrame, on the other hand, has a built-in option for implicit parallelism: EnableImplicitMT(). However, it
        appears to work inconsistently on different machines and different ROOT versions.
      </p>

      <figure>
        <img src="plots/implicitmt1.png">
      </figure>
      <figure>
        <img src="plots/implicitmt3.png">
      </figure>
      <figure>
        <img src="plots/implicitmt2.png">
      </figure>

    </article>

    <article property="abstract">
      <header>
        <h3>Parallel processing with RDataFrame </h3>
      </header>

      <p>
        Alternatively, we followed the same approach mentioned above: explicitly creating subprocesses to handle pieces
        of data. Unlike Uproot,
        RDataFrame doesn’t provide the option to specify the number of events to be processed in each file, so the
        smallest unit of data distributed to each subprocess is one
        file. As a result, the runtime function for RDataFrame would be:
      </p>

      \[ T = C(S, P) + t_f\cdot \lceil{S_f \over P}\rceil \]

      <p>
        Where t<sub>f</sub> is the time to process one <strong>ROOT file</strong>, and S<sub>f</sub> is number of
        <strong>ROOT files</strong> in the dataset.
      </p>

      <p>
        If the files had different sizes, t<sub>f</sub> would vary, making it difficult to understand the performance of
        RDataFrame. To prevent this, therefore, we split the original dataset
        into 128 files of the same size. For a fair comparison, we used these files as input for both Uproot and
        RDataFrame.
      </p>

    </article>

    <article property="abstract">
      <header>
        <h3>Results - Local and remote access</h3>
      </header>

      <figure>
        <img src="plots/localvsremote1.png">
      </figure>
      <figure>
        <img src="plots/localvsremote2.png">
      </figure>

      <p>
        We found that accessing the data on local disk resulted in an insignificant increase in
        performance. In all subsequent measurements, therefore, data were always accessed remotely.
      </p>

    </article>

    <article property="abstract">
      <header>
        <h3>Results - Runtime vs Processes</h3>
      </header>

      <p>
        By keeping the size of data constant (128 files/128GB/4.5 &#183; 10<sup>8</sup> events) and varying the number
        of processes (at 1 process, there is no parallelism), we
        obtained the following results:
      </p>

      <figure>
        <img src="plots/rvp1.png">
      </figure>
      <figure>
        <img src="plots/rvp2.png">
      </figure>
      <figure>
        <img src="plots/rvp3.png">
      </figure>

      <p>
        The plots, which show that runtime is inversely proportional to the number of process, are consistent with the
        runtime functions mentioned above, as we found in other tests that the time C(S, P) is constant. While the
        curves of Uproot
        are fairly
        smooth, we can see a discrete pattern in RDataFrame's plots. This reflects the fact that RDataFrame's smallest
        unit of data is one file, which is much greater than Uproot's smallest unit of data (one event).
      </p>

    </article>

    <article property="abstract">
      <header>
        <h3>Results - Speed-up vs Processes</h3>
      </header>
      <p>
        From the previous plots, we can calculate how much speed-up we gained by using parallel processing. Speed-up at
        n
        processes is the ratio between the runtime at 1 process and the runtime at n processes.
      </p>
      <figure>
        <img src="plots/svp1.png">
      </figure>
      <figure>
        <img src="plots/svp2.png">
      </figure>
      <figure>
        <img src="plots/svp3.png">
      </figure>

      <p>
        From the plots, it can be observed that we gained a significant performance boost with both Uproot and
        RDataFrame by implementing parallel processing. On the machine wn-gpu-8-3-22, for instance, Uproot with
        multiprocessing could be almost 9 times faster than normal. In the case of RDataFrame, we achieved an even more
        striking speed-up: at 128 subprocesses, it was over 50 times faster than without parallelism.
      </p>

      <p>
        Expectedly, we didn’t gain anything when we
        created more subprocesses than the number of CPUs on the machine. This was clearly visible on wn-1-8-9 and
        tesla04, which have 64 and 32 CPUs, respectively. With Uproot, however, the performance boost peaked at around
        32 processes, no matter how many CPUs we had. This was because for Uproot, C(S, P) was large, and as a result,
        it
        dominated the speed-up gained by creating
        more processes. In the case of RDataFrame, C(S, P) is relatively insignificant, so we gained performance boost
        until the maximum number of CPUs is reached.
      </p>
    </article>

    <article property="abstract">
      <header>
        <h3>Results - Runtime vs Size</h3>
      </header>
      <p>
        By keeping the number
        of processes constant (at 32 or 64) and varying the size of data, we
        obtained the following results:
      </p>
      <figure>
        <img src="plots/rvs1.png">
      </figure>
      <figure>
        <img src="plots/rvs2.png">
      </figure>
      <figure>
        <img src="plots/rvs3.png">
      </figure>

      <p>
        The plots show that the total runtime is linearly proportional to the size of data, which is what we expected.
        In RDataFrame's curves, a step-like pattern is observed, which again reflects that with our approach, RDataFrame
        parallelized over files instead of events.
      </p>

    </article>

    <article property="abstract">
      <header>
        <h3>Conclusion</h3>
      </header>

      <p>
        By enabling parallelism, we effectively optimized the performance of both Uproot and RDataFrame. While Uproot
        ran faster at fewer processes, RDataFrame got better performance as the number of processes increased. This may
        suggest that on machines with a lot of CPUs, it is more beneficial to use the latter. However, we must keep in
        mind that RDataFrame was able to reach such a great performance because the original dataset was divided into
        many small files with the same number of events. In real-life situations, such condition is not guaranteed, so
        Uproot may still be the better choice.
      </p>

    </article>

    <!-- <figure style="flex-grow: 9999999">
      <img style="width: 70%" src="logo.svg" alt="Project logo" />
      <figcaption>A stand-alone figure to fill the remaining space.</figcaption>
    </figure> -->
  </main>

  <footer>
    <address class="monospace">
      hoadh-23@rhodes.edu
    </address>
    <address class="monospace">
      adriano.diflorio@ba.infn.it
    </address>
    <address class="monospace">
      alexis.pompili@ba.infn.it
    </address>
    <address class="monospace">
      umit.sozbilir@cern.ch
    </address>
    <address class="monospace">
      vin.mas21@gmail.com
    </address>
    <!-- <span class="credits">
    </span> -->
  </footer>
</body>

</html>